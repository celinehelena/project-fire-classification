# üî• Classifica√ß√£o Bin√°ria da Presen√ßa de Fogo em Imagens com Yolov11
Este reposit√≥rio tem por objetivo apresentar um projeto desenvolvido para a disciplina de gradua√ß√£o, Vis√£o Computacional, que consiste em aborda uma solu√ß√£o para a classifica√ß√£o da presen√ßa de fogo em imagens usando o YOLOv11.

## üìå Introdu√ß√£o
Em 2024, o Brasil registrou 278,3 mil focos de inc√™ndio, segundo o Inpe, que representa um aumento de 46,5%, em rela√ß√£o ao ano anterior. De modo que a detec√ß√£o precoce desses principios de inc√™ndio √© essencia para uma resposta r√°pida e eficaz, minimizando os danos causados. Para que isso aconte√ßa √© necess√°rio um monitoramento autom√°tico de inc√™ndio por meio de imagens. Nesse contexto, a aplica√ß√£o de solu√ß√µes baseadas em vis√£o computacional e aprendizado de m√°quina tem se mostrado eficiente para a detec√ß√£o precoce de focos de inc√™ndio em imagens capturadas por sat√©lites, c√¢meras fixas ou drones.

Este trabalho apresenta o desenvolvimento de um aplica√ß√£o de classifica√ß√£o de imagens que identifica a presen√ßa ou aus√™ncia de fogo, utilizando a arquitetura YOLOv11, um modelo especifico para classifica√ß√£o de imagens. O objetivo √© treinar esse modelo para classificar imagens em duas categorias: "fire" e "non_fire".

## ‚öôÔ∏è Metodologia
Para a realiza√ß√£o da tarefa de classifica√ß√£o bin√°ria de imagens, foi utilizado o [Fire Dataset](https://www.kaggle.com/datasets/phylake1337/fire-dataset), disponibilizado na plataforma Kaggle. O conjunto de dados √© composto por 999 imagens divididas em duas classes: `fire_images` (imagens contendo fogo) e `non_fire_images` (imagens sem ocorr√™ncia de fogo). As imagens contemplam uma ampla gama de cen√°rios, incluindo ambientes naturais (como florestas) e urbanos (como edifica√ß√µes, resid√™ncias, ve√≠culos e rodovias), o que contribui para uma maior robustez na generaliza√ß√£o do modelo.
A seguir, √© exibida uma amostra representativa do dataset, gerada por um script automatizado respons√°vel por selecionar imagens aleat√≥rias da base.
<p align="center">
<img src="assets/amostra_dataset_img.png" alt="Amostra do Dataset" width="450"/>
</p>

### üóÇÔ∏è Distribui√ß√£o do Dataset
O processo teve in√≠cio com o download da base de dados e sua reorganiza√ß√£o no formato exigido pelo YOLO, com as imagens distribu√≠das em subpastas para treinamento, valida√ß√£o e teste. O conjunto original foi, ent√£o, dividido aleatoriamente em tr√™s subconjuntos: treinamento (70%), valida√ß√£o (15%) e teste (15%). O gr√°fico abaixo ilustra essa divis√£o:
<p align="center">
<img src="assets/distribution_dataset_img.png" alt="Gr√°fico de Distribui√ß√£o das Imagens no Dataset" width="450"/>
</p>

A divis√£o foi realizada com aux√≠lio de um script que aloca aleatoriamente as imagens entre os tr√™s subconjuntos. A distribui√ß√£o final das imagens ficou da seguinte forma:

| Classe     | Treino | Valida√ß√£o | Teste |
|------------|--------|-----------|-------|
| `fire`     | 528    | 113       | 114   |
| `non_fire` | 170    | 36        | 38    |

### Pr√©-Processamento de dados

As imagens passaram por um processo de pr√©-processamento, sendo automaticamente redimensionadas para 640√ó640 pixels, conforme o par√¢metro `imgsz`, e normalizadas com os valores de pixel convertidos para o intervalo `[0, 1]`, como exigido pelo pipeline da Ultralytics. T√©cnicas leves de data augmentation foram aplicadas automaticamente, incluindo espelhamento horizontal aleat√≥rio, corte e altera√ß√µes de brilho e contraste, contribuindo para uma melhor capacidade de generaliza√ß√£o do modelo.

Inclui tamb√©m no pr√©-processamente a etapa mencionada anteriormente, particionamento do conjunto de dados em treinamento (70%), valida√ß√£o (15%) e teste (15%).

### Arquitetura do modelo
A arquitetura selecionada para a tarefa de classifica√ß√£o de imagens foi a YOLOv11 (You Only Look Once, vers√£o 11), com √™nfase no modelo `yolo11n-cls.pt`, previamente treinado para tarefas de classifica√ß√£o. Esta vers√£o √© adaptada para classifica√ß√£o pura e mostra-se adequada para cen√°rios com dados limitados e distribui√ß√£o desbalanceada entre as classes.

A tarefa desenvolvida consistiu em uma classifica√ß√£o bin√°ria, com o objetivo de identificar a presen√ßa (fire) ou aus√™ncia (non_fire) de fogo em imagens. O modelo foi refinado para capturar padr√µes visuais caracter√≠sticos de inc√™ndios, como a presen√ßa de chamas, colora√ß√£o avermelhada e fuma√ßa.

### **Ferramentas e Bibliotecas Utilizadas**

O desenvolvimento e a execu√ß√£o do projeto foram realizados com o suporte das seguintes ferramentas e bibliotecas:
* **YOLOv11 (Ultralytics):** Framework utilizado para implementa√ß√£o da arquitetura e gerenciamento da pipeline de treinamento;
* **PyTorch:** Biblioteca de aprendizado profundo empregada na modelagem e execu√ß√£o da rede neural;
* **Python 3.10:** Linguagem de programa√ß√£o principal utilizada no projeto;
* **Google Colab (GPU Tesla T4):** Ambiente de desenvolvimento com suporte √† acelera√ß√£o por GPU, facilitando o treinamento do modelo;
* **Matplotlib**: Bibliotecas para visualiza√ß√£o de dados e an√°lise gr√°fica dos resultados.
  
Essa infraestrutura possibilitou um fluxo de trabalho eficiente e reprodut√≠vel, al√©m de oferecer suporte para o monitoramento detalhado do desempenho do modelo.

### Treinamento e Avalia√ß√£o do Modelo

O treinamento foi conduzido usando os seguintes principais par√¢metros:
* `model='yolo11n-cls.pt'` ‚Äì Modelo base pr√©-treinado para classifica√ß√£o;
* `data=data.yaml` ‚Äì Arquivo de configura√ß√£o com caminhos para os conjuntos train, val e test;
* `epochs=25` ‚Äì N√∫mero total de √©pocas para treinamento;
* `batch=16` ‚Äì Tamanho do lote (batch size);
* `imgsz=640` ‚Äì Resolu√ß√£o das imagens (640x640 pixels);

Durante o treinamento, o modelo foi capaz de aprender rapidamente os padr√µes visuais relacionados √† presen√ßa de fogo, com melhora significativa das m√©tricas j√° nas primeiras √©pocas. 
<div align="center"> <img src="assets/train/results.png" alt="Imagens da etapa de treinamento" width="400"/> </div>

Ap√≥s o treinamento, a performance do modelo foi avaliada automaticamente com base nos seguintes indicadores:

* **Loss** ‚Äì Perda (erro) da predi√ß√£o ao longo das √©pocas;
* **Top-1 Accuracy** ‚Äì Percentual de acertos considerando a classe mais prov√°vel;
* **Top-5 Accuracy** ‚Äì Percentual de acertos considerando as 5 classes mais prov√°veis (embora a tarefa seja bin√°ria);
* **Confusion Matrix** ‚Äì Representa√ß√£o visual dos acertos e erros por classe.

Al√©m disso, o modelo foi testado sobre o conjunto separado de imagens (`test`) para avaliar sua capacidade de generaliza√ß√£o, e os erros cometidos foram analisados individualmente, com imagens incorretamente classificadas sendo exibidas para an√°lise qualitativa.

## üìä Resultados e Discuss√µes
Ao final do treinamento, o modelo **YOLOv11** apresentou desempenho satisfat√≥rio, com taxa de erro residual m√≠nima. A figura a seguir mostra a evolu√ß√£o das curvas de loss e acur√°cia ao longo das √©pocas. Nota-se que o **loss** de treino teve uma queda consistente, enquanto o de valida√ß√£o caiu rapidamente nas primeiras √©pocas e estabilizou-se pr√≥ximo de zero. J√° a **acur√°cia top-1** evoluiu positivamente, superando 97%, enquanto a **acur√°cia top-5** manteve-se constante em 100%.

<div align="center"> <img src="assets/train/results.png" alt="Curvas de Acur√°cia e Perda" width="400"/> </div>

A acur√°cia final no conjunto de treinamento foi de **98,7%**. Para avaliar o desempenho ap√≥s o treinamento, a rede foi testada com o conjunto de teste, e a matriz de confus√£o obtida demonstra a seguinte performance:
<div align="center"> <img src="assets/test/confusion_matrix.png" alt="Matriz de Confus√£o" width="400"/> </div>

A partir da matriz, observa-se que o modelo classificou corretamente 114 de 115 imagens da classe `fire` e 36 de 38 da classe `non_fire`, o que corresponde a uma acur√°cia de aproximadamente **94,7%**. Esses resultados indicam uma excelente capacidade de generaliza√ß√£o para imagens n√£o vistas anteriormente. Considerando a divis√£o aleat√≥ria do dataset, o modelo mostrou-se robusto, com √≥timo desempenho mesmo em cen√°rios variados de ilumina√ß√£o e perspectiva.

Al√©m disso, foi realizada uma compara√ß√£o entre as matrizes de confus√£o dos conjuntos de treinamento e teste, como mostrado a seguir:
<div align="center"> <table> <tr> <td align="center"><strong>Treinamento</strong><br><img src="assets/train/confusion_matrix.png" alt="Matriz de Treinamento" width="700"/></td> <td align="center"><strong>Teste</strong><br><img src="assets/test/confusion_matrix.png" alt="Matriz de Teste" width="700"/></td> </tr> </table> </div>
A seguir, √© apresentada uma imagem com alguns dos resultados de classifica√ß√£o do modelo:
<div align="center"> <img src="assets/test/confusion_matrix.png" alt="Matriz de Confus√£o" width="700"/> </div>

A an√°lise mostra que o modelo manteve desempenho semelhante entre as fases, com uma leve oscila√ß√£o nos falsos positivos e falsos negativos. A aus√™ncia de predi√ß√µes para a classe background em ambas as fases sugere a possibilidade de reavaliar a representa√ß√£o dessa categoria nas anota√ß√µes ou sua real necessidade no modelo.

A seguir, √© apresentada uma imagem com alguns dos resultados de classifica√ß√£o do modelo:

<div align="center"> <img src="assets/test/predictions.png" alt="Predi√ß√µes em imagens reais" width="400"/> </div>

Foi identificado apenas um caso de erro de classifica√ß√£o, ilustrado na imagem abaixo.

<div align="center"> <img src="assets/test/misclassified_example.png" alt="Erro de classifica√ß√£o" width="400"/> </div>

## Conclus√£o
Os resultados alcan√ßados foram poss√≠veis devido a utiliza√ß√£o do YOLOv11 na tarefa de classifica√ß√£o aliado ao otimizador Adam, permitindo encontrar um desempenho robusto sem a necessidade de arquiteturas complexas ou alto custo computacional. Em trabalhos futuros, pode-se expandir essa aplica√ß√£o para a detec√ß√£o localizada de focos de inc√™ndio em tempo real (objeto + bounding box), incorporando varia√ß√µes clim√°ticas e contextos diversos. 

